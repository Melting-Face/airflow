{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark =  SparkSession.builder.getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639\n",
      "1917\n",
      "+---------+-------+-------------------+-----------------+\n",
      "|code_from|code_to|classification_from|classification_to|\n",
      "+---------+-------+-------------------+-----------------+\n",
      "|    TOTAL|  TOTAL|                 H6|               H4|\n",
      "|       01|     01|                 H6|               H4|\n",
      "|     0101|   0101|                 H6|               H4|\n",
      "|     0102|   0102|                 H6|               H4|\n",
      "|     0103|   0103|                 H6|               H4|\n",
      "|     0104|   0104|                 H6|               H4|\n",
      "|     0105|   0105|                 H6|               H4|\n",
      "|     0106|   0106|                 H6|               H4|\n",
      "|       02|     02|                 H6|               H4|\n",
      "|     0201|   0201|                 H6|               H4|\n",
      "|     0202|   0202|                 H6|               H4|\n",
      "|     0203|   0203|                 H6|               H4|\n",
      "|     0204|   0204|                 H6|               H4|\n",
      "|     0205|   0205|                 H6|               H4|\n",
      "|     0206|   0206|                 H6|               H4|\n",
      "|     0207|   0207|                 H6|               H4|\n",
      "|     0208|   0208|                 H6|               H4|\n",
      "|     0209|   0209|                 H6|               H4|\n",
      "|     0210|   0210|                 H6|               H4|\n",
      "|       03|     03|                 H6|               H4|\n",
      "+---------+-------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "  \"classification_from\": ['H6', 'H5', 'H4'],\n",
    "  \"classification_to\": ['H4', 'H4', 'H4'],\n",
    "})\n",
    "\n",
    "classification_DF = spark.createDataFrame(df)\n",
    "\n",
    "DF = spark.read.parquet(\"HS.parquet\")\n",
    "DF = DF.where(\n",
    "  (F.col(\"aggrLevel\") == 0)\n",
    "  | (F.col(\"aggrLevel\") == 2)\n",
    "  | (F.col(\"aggrLevel\") == 4)\n",
    ")\n",
    "\n",
    "DF = DF.where(\n",
    "  (F.col(\"id\") == \"TOTAL\")\n",
    "  | (F.col(\"id\").rlike(\"^[0-4]\"))\n",
    ")\n",
    "\n",
    "DF = DF.withColumns({\n",
    "  \"code_from\": F.col(\"id\"),\n",
    "  \"code_to\": F.col(\"id\"),\n",
    "})\n",
    "\n",
    "DF = DF.select([\"code_from\", \"code_to\"])\n",
    "print(DF.count())\n",
    "DF = DF.crossJoin(classification_DF)\n",
    "print(DF.count())\n",
    "DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+---------+-------+\n",
      "|classification_from|classification_to|code_from|code_to|\n",
      "+-------------------+-----------------+---------+-------+\n",
      "|                 H6|               H4|   010121| 010121|\n",
      "|                 H6|               H4|   010129| 010129|\n",
      "|                 H6|               H4|   010130| 010130|\n",
      "|                 H6|               H4|   010190| 010190|\n",
      "|                 H6|               H4|   010221| 010221|\n",
      "|                 H6|               H4|   010229| 010229|\n",
      "|                 H6|               H4|   010231| 010231|\n",
      "|                 H6|               H4|   010239| 010239|\n",
      "|                 H6|               H4|   010290| 010290|\n",
      "|                 H6|               H4|   010310| 010310|\n",
      "|                 H6|               H4|   010391| 010391|\n",
      "|                 H6|               H4|   010392| 010392|\n",
      "|                 H6|               H4|   010410| 010410|\n",
      "|                 H6|               H4|   010420| 010420|\n",
      "|                 H6|               H4|   010511| 010511|\n",
      "|                 H6|               H4|   010512| 010512|\n",
      "|                 H6|               H4|   010513| 010513|\n",
      "|                 H6|               H4|   010514| 010514|\n",
      "|                 H6|               H4|   010515| 010515|\n",
      "|                 H6|               H4|   010594| 010594|\n",
      "+-------------------+-----------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_2022 = spark.read.parquet(\"HS2022_2012.parquet\")\n",
    "DF_2022 = DF_2022.withColumns({\n",
    "  \"classification_from\": F.lit(\"H6\"),\n",
    "  \"classification_to\": F.lit(\"H4\"),\n",
    "  \"code_from\": F.col(\"HS2022\"),\n",
    "  \"code_to\": F.col(\"HS2012\"),\n",
    "})\n",
    "DF_2022 = DF_2022.select([\n",
    "  \"classification_from\",\n",
    "  \"classification_to\",\n",
    "  \"code_from\",\n",
    "  \"code_to\",\n",
    "])\n",
    "DF_2022.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+---------+-------+\n",
      "|classification_from|classification_to|code_from|code_to|\n",
      "+-------------------+-----------------+---------+-------+\n",
      "|                 H5|               H4|   010121| 010121|\n",
      "|                 H5|               H4|   010129| 010129|\n",
      "|                 H5|               H4|   010130| 010130|\n",
      "|                 H5|               H4|   010190| 010190|\n",
      "|                 H5|               H4|   010221| 010221|\n",
      "|                 H5|               H4|   010229| 010229|\n",
      "|                 H5|               H4|   010231| 010231|\n",
      "|                 H5|               H4|   010239| 010239|\n",
      "|                 H5|               H4|   010290| 010290|\n",
      "|                 H5|               H4|   010310| 010310|\n",
      "|                 H5|               H4|   010391| 010391|\n",
      "|                 H5|               H4|   010392| 010392|\n",
      "|                 H5|               H4|   010410| 010410|\n",
      "|                 H5|               H4|   010420| 010420|\n",
      "|                 H5|               H4|   010511| 010511|\n",
      "|                 H5|               H4|   010512| 010512|\n",
      "|                 H5|               H4|   010513| 010513|\n",
      "|                 H5|               H4|   010514| 010514|\n",
      "|                 H5|               H4|   010515| 010515|\n",
      "|                 H5|               H4|   010594| 010594|\n",
      "+-------------------+-----------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_2017 = spark.read.parquet(\"HS2017_2012.parquet\")\n",
    "DF_2017 = DF_2017.withColumns({\n",
    "  \"classification_from\": F.lit(\"H5\"),\n",
    "  \"classification_to\": F.lit(\"H4\"),\n",
    "  \"code_from\": F.col(\"HS2017\"),\n",
    "  \"code_to\": F.col(\"HS2012\"),\n",
    "})\n",
    "DF_2017 = DF_2017.select([\n",
    "  \"classification_from\",\n",
    "  \"classification_to\",\n",
    "  \"code_from\",\n",
    "  \"code_to\",\n",
    "])\n",
    "DF_2017.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------------+-----------------+\n",
      "|code_from|code_to|classification_from|classification_to|\n",
      "+---------+-------+-------------------+-----------------+\n",
      "|    TOTAL|  TOTAL|                 H6|               H4|\n",
      "|    TOTAL|  TOTAL|                 H5|               H4|\n",
      "|    TOTAL|  TOTAL|                 H4|               H4|\n",
      "+---------+-------+-------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22702"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = DF.unionAll(DF_2022)\n",
    "DF = DF.unionAll(DF_2017)\n",
    "DF.write.\n",
    "DF.where(F.col(\"code_from\") == \"TOTAL\").show()\n",
    "DF.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
